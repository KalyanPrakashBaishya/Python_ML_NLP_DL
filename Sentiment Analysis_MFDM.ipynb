{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\kalya\\\\Python'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enabling print for all lines\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Checking the working directory\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing necessary libraries\n",
    "#!pip install vaderSentiment\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import sklearn\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\kalya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kalya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading vader and stopwords\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Different types of encoding which could be tried are 'latin1', 'iso-8859-1', 'cp1252', 'utf-8' etc`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name                        0.000000\n",
       "Response ID                 0.000000\n",
       "Date collected              0.000000\n",
       "VisitorId (Tealeaf)         0.304947\n",
       "Tealeaf Link                0.508245\n",
       "Logic: Page collected on    0.225887\n",
       "Virtual: Device             5.692342\n",
       "Issue Area                  0.056472\n",
       "Issue Experienced           0.000000\n",
       "Sub Reason                  0.000000\n",
       "Problem statement           0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the data set\n",
    "# text = pd.read_csv(\"feedback_from_apr_aug_2018.csv\", usecols = ['Issue Area','Issue Experienced'], encoding = 'latin').dropna()\n",
    "text = pd.read_csv(\"feedback_from_apr_aug_2018.csv\", encoding = 'latin')\n",
    "# text\n",
    "text1= text['Issue Experienced'].dropna()\n",
    "# text1.head()\n",
    "# text.info()\n",
    "text.isnull().sum()*100/len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Sentiment Analysis using VADER](https://github.com/cjhutto/vaderSentiment)**\n",
    "\n",
    "*`VADER(Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media`*\n",
    "\n",
    "Recent updates in VADER can handle:\n",
    "\n",
    "    > typical negations (e.g., \"not good\")\n",
    "    > use of contractions as negations (e.g., \"wasn't very good\")\n",
    "    > conventional use of punctuation to signal increased sentiment intensity (e.g., \"Good!!!\")\n",
    "    > conventional use of word-shape to signal emphasis (e.g., using ALL CAPS for words/phrases)\n",
    "    > using degree modifiers to alter sentiment intensity (e.g., intensity boosters such as \"very\" and intensity dampeners such as \"kind of\")\n",
    "    > understanding many sentiment-laden slang words (e.g., 'sux')\n",
    "    > understanding many sentiment-laden slang words as modifiers such as 'uber' or 'friggin' or 'kinda'\n",
    "    > understanding many sentiment-laden emoticons such as :) and :D\n",
    "    > translating utf-8 encoded emojis such as ðŸ’˜ and ðŸ’‹ and ðŸ˜\n",
    "    > understanding sentiment-laden initialisms and acronyms (for example: 'lol')\n",
    "    \n",
    "    \n",
    "[Refer this article](https://medium.com/analytics-vidhya/simplifying-social-media-sentiment-analysis-using-vader-in-python-f9e6ec6fc52f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Issue Area            2118\n",
       "Issue Experienced     2118\n",
       "Polarity_score        2118\n",
       "sentiment_category    2118\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Issue Area            3004\n",
       "Issue Experienced     3006\n",
       "Polarity_score        3006\n",
       "sentiment_category    3006\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Issue Area            3727\n",
       "Issue Experienced     3730\n",
       "Polarity_score        3730\n",
       "sentiment_category    3730\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a function fpr generating polarity scores\n",
    "senti = SentimentIntensityAnalyzer()\n",
    "text_polar = lambda x: senti.polarity_scores(x)['compound']\n",
    "\n",
    "# Generating the polarity score on the 'Issue Experienced' variable\n",
    "text['Polarity_score'] = text['Issue Experienced'].apply(text_polar)\n",
    "text_final= text[['Issue Area','Issue Experienced','Polarity_score']]\n",
    "# text_final.head()\n",
    "\n",
    "# Categorizing the polarity\n",
    "sentiment_mapping = {1: \"High Negative\", 2: \"Negative\", 3: \"Neutral\", 4:\"Positive\", 5:\"High Positive\"}\n",
    "map_sentiment = lambda val : np.digitize(val,[-1,-.8,-.25,.25,.8])\n",
    "text_final['sentiment_category'] = text_final['Polarity_score'].apply(map_sentiment)\n",
    "# text_final.head()\n",
    "\n",
    "# Count of polarity per Issue Area\n",
    "positives_count = text_final[(text_final.sentiment_category == 4) |(text_final.sentiment_category == 5)].count()\n",
    "positives_count\n",
    "negatives_count = text_final[(text_final.sentiment_category == 2) |(text_final.sentiment_category == 1)].count()\n",
    "negatives_count\n",
    "neutral_count = text_final[(text_final.sentiment_category == 3)].count()\n",
    "neutral_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Issue Experienced</th>\n",
       "      <th>Clean_text</th>\n",
       "      <th>text_without_stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Not use friendly</td>\n",
       "      <td>not use friendly</td>\n",
       "      <td>use  friendly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>I purchased a jacket T193787.when I got the ja...</td>\n",
       "      <td>i purchased a jacket twhen i got the jacket ho...</td>\n",
       "      <td>purchased  jacket  twhen  got  jacket  home  s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>I am looking for briefs for my 15 year old son...</td>\n",
       "      <td>i am looking for briefs for my  year old son h...</td>\n",
       "      <td>looking  briefs  year  old  son  however  coul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Please can you re stock item t43/6079 Colour I...</td>\n",
       "      <td>please can you re stock item t colour ivory si...</td>\n",
       "      <td>please  stock  item  colour  ivory  size  cons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>My order has not been delivered , although you...</td>\n",
       "      <td>my order has not been delivered  although your...</td>\n",
       "      <td>order  delivered  although  tracking  say  dis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Issue Experienced  \\\n",
       "0                                   Not use friendly   \n",
       "1  I purchased a jacket T193787.when I got the ja...   \n",
       "2  I am looking for briefs for my 15 year old son...   \n",
       "3  Please can you re stock item t43/6079 Colour I...   \n",
       "4  My order has not been delivered , although you...   \n",
       "\n",
       "                                          Clean_text  \\\n",
       "0                                   not use friendly   \n",
       "1  i purchased a jacket twhen i got the jacket ho...   \n",
       "2  i am looking for briefs for my  year old son h...   \n",
       "3  please can you re stock item t colour ivory si...   \n",
       "4  my order has not been delivered  although your...   \n",
       "\n",
       "                                   text_without_stop  \n",
       "0                                      use  friendly  \n",
       "1  purchased  jacket  twhen  got  jacket  home  s...  \n",
       "2  looking  briefs  year  old  son  however  coul...  \n",
       "3  please  stock  item  colour  ivory  size  cons...  \n",
       "4  order  delivered  although  tracking  say  dis...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning the data set required for pre-processing\n",
    "\n",
    "# Text to lower\n",
    "text['Clean_text'] = text1.str.lower().str.replace('[^a-z ]','')\n",
    "# text['Clean_text'].head()\n",
    "\n",
    "# Removing stopwords\n",
    "stop=stopwords.words('english')\n",
    "def sw(text):\n",
    "    text=[word for word in str(text).split() if word not in stop]\n",
    "    return \"  \".join(text)\n",
    "\n",
    "text['text_without_stop']=text['Clean_text'].apply(sw)\n",
    "# print(text['Clean_text'][1])\n",
    "# print(text['text_without_stop'][1])\n",
    "\n",
    "# Checking the modified data set\n",
    "text[['Issue Experienced','Clean_text','text_without_stop']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DTM using TF-IDF Vectorizer\n",
    "vec = TfidfVectorizer()\n",
    "DTM = vec.fit_transform(text['text_without_stop'])\n",
    "\n",
    "# Understand the parameters of lDA\n",
    "lda = LatentDirichletAllocation(n_components=5, max_iter=10, random_state = 1234)\n",
    "lda_output = lda.fit_transform(DTM)\n",
    "# print(lda)\n",
    "topic_name = ['Topic' + str(i) for i in range(lda.n_components)]\n",
    "docname = ['Doc' + str(i) for i in range(DTM.shape[0])]\n",
    "# lda_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aanddharveygmailcom</th>\n",
       "      <th>aaron</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abbot</th>\n",
       "      <th>abcbcdaa</th>\n",
       "      <th>abd</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>...</th>\n",
       "      <th>zillions</th>\n",
       "      <th>zim</th>\n",
       "      <th>zinc</th>\n",
       "      <th>zinder</th>\n",
       "      <th>zip</th>\n",
       "      <th>zipped</th>\n",
       "      <th>zips</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Topic0</td>\n",
       "      <td>0.202024</td>\n",
       "      <td>0.317356</td>\n",
       "      <td>0.204714</td>\n",
       "      <td>0.200025</td>\n",
       "      <td>1.054194</td>\n",
       "      <td>0.787050</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.201407</td>\n",
       "      <td>0.327840</td>\n",
       "      <td>0.200011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.212515</td>\n",
       "      <td>0.610059</td>\n",
       "      <td>0.200013</td>\n",
       "      <td>0.200019</td>\n",
       "      <td>0.204422</td>\n",
       "      <td>0.200023</td>\n",
       "      <td>0.205750</td>\n",
       "      <td>0.202227</td>\n",
       "      <td>0.200015</td>\n",
       "      <td>0.200008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic1</td>\n",
       "      <td>0.822118</td>\n",
       "      <td>0.200126</td>\n",
       "      <td>0.371319</td>\n",
       "      <td>0.224999</td>\n",
       "      <td>0.200067</td>\n",
       "      <td>0.200034</td>\n",
       "      <td>0.200035</td>\n",
       "      <td>0.200016</td>\n",
       "      <td>0.556301</td>\n",
       "      <td>0.200029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200037</td>\n",
       "      <td>0.200076</td>\n",
       "      <td>0.200033</td>\n",
       "      <td>0.200054</td>\n",
       "      <td>0.200052</td>\n",
       "      <td>0.200056</td>\n",
       "      <td>0.200033</td>\n",
       "      <td>0.275684</td>\n",
       "      <td>0.200031</td>\n",
       "      <td>0.200019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic2</td>\n",
       "      <td>0.200106</td>\n",
       "      <td>0.200239</td>\n",
       "      <td>0.200027</td>\n",
       "      <td>0.200110</td>\n",
       "      <td>0.200096</td>\n",
       "      <td>0.200058</td>\n",
       "      <td>0.200055</td>\n",
       "      <td>0.521690</td>\n",
       "      <td>0.414257</td>\n",
       "      <td>0.200052</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200066</td>\n",
       "      <td>0.200085</td>\n",
       "      <td>0.200059</td>\n",
       "      <td>0.200076</td>\n",
       "      <td>0.200127</td>\n",
       "      <td>0.400315</td>\n",
       "      <td>0.200064</td>\n",
       "      <td>0.200026</td>\n",
       "      <td>0.200057</td>\n",
       "      <td>0.200034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic3</td>\n",
       "      <td>0.200101</td>\n",
       "      <td>0.223418</td>\n",
       "      <td>0.200023</td>\n",
       "      <td>0.200102</td>\n",
       "      <td>0.200103</td>\n",
       "      <td>0.200055</td>\n",
       "      <td>0.200055</td>\n",
       "      <td>0.200028</td>\n",
       "      <td>0.870362</td>\n",
       "      <td>0.200057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.492536</td>\n",
       "      <td>0.200081</td>\n",
       "      <td>0.200055</td>\n",
       "      <td>0.200074</td>\n",
       "      <td>0.200043</td>\n",
       "      <td>0.200102</td>\n",
       "      <td>0.201578</td>\n",
       "      <td>0.200026</td>\n",
       "      <td>0.200054</td>\n",
       "      <td>0.200032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic4</td>\n",
       "      <td>0.209618</td>\n",
       "      <td>0.283087</td>\n",
       "      <td>0.201626</td>\n",
       "      <td>0.392831</td>\n",
       "      <td>0.689511</td>\n",
       "      <td>0.200659</td>\n",
       "      <td>0.201425</td>\n",
       "      <td>0.200006</td>\n",
       "      <td>0.207490</td>\n",
       "      <td>0.348632</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200016</td>\n",
       "      <td>0.200022</td>\n",
       "      <td>0.365430</td>\n",
       "      <td>0.640357</td>\n",
       "      <td>2.119057</td>\n",
       "      <td>0.218760</td>\n",
       "      <td>0.815739</td>\n",
       "      <td>0.202040</td>\n",
       "      <td>0.553888</td>\n",
       "      <td>0.944761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 10703 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              aa  aanddharveygmailcom     aaron   abandon  abandoned  \\\n",
       "Topic0  0.202024             0.317356  0.204714  0.200025   1.054194   \n",
       "Topic1  0.822118             0.200126  0.371319  0.224999   0.200067   \n",
       "Topic2  0.200106             0.200239  0.200027  0.200110   0.200096   \n",
       "Topic3  0.200101             0.223418  0.200023  0.200102   0.200103   \n",
       "Topic4  0.209618             0.283087  0.201626  0.392831   0.689511   \n",
       "\n",
       "        abandoning     abbot  abcbcdaa       abd   abdomen  ...  zillions  \\\n",
       "Topic0    0.787050  0.769231  0.201407  0.327840  0.200011  ...  0.212515   \n",
       "Topic1    0.200034  0.200035  0.200016  0.556301  0.200029  ...  0.200037   \n",
       "Topic2    0.200058  0.200055  0.521690  0.414257  0.200052  ...  0.200066   \n",
       "Topic3    0.200055  0.200055  0.200028  0.870362  0.200057  ...  0.492536   \n",
       "Topic4    0.200659  0.201425  0.200006  0.207490  0.348632  ...  0.200016   \n",
       "\n",
       "             zim      zinc    zinder       zip    zipped      zips   zombies  \\\n",
       "Topic0  0.610059  0.200013  0.200019  0.204422  0.200023  0.205750  0.202227   \n",
       "Topic1  0.200076  0.200033  0.200054  0.200052  0.200056  0.200033  0.275684   \n",
       "Topic2  0.200085  0.200059  0.200076  0.200127  0.400315  0.200064  0.200026   \n",
       "Topic3  0.200081  0.200055  0.200074  0.200043  0.200102  0.201578  0.200026   \n",
       "Topic4  0.200022  0.365430  0.640357  2.119057  0.218760  0.815739  0.202040   \n",
       "\n",
       "            zone      zoom  \n",
       "Topic0  0.200015  0.200008  \n",
       "Topic1  0.200031  0.200019  \n",
       "Topic2  0.200057  0.200034  \n",
       "Topic3  0.200054  0.200032  \n",
       "Topic4  0.553888  0.944761  \n",
       "\n",
       "[5 rows x 10703 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building a data frame out of the topics created\n",
    "df_document_topic = pd.DataFrame(np.round(lda_output,2),columns= topic_name, index = docname)\n",
    "# df_document_topic.head()\n",
    "\n",
    "# Dominatung topic\n",
    "dominating_topic = np.argmax(df_document_topic.values, axis =1)\n",
    "# dominating_topic\n",
    "df_document_topic['dominating_topic'] = dominating_topic\n",
    "# df_document_topic.head()\n",
    "# df_document_topic.groupby('dominating_topic').size()\n",
    "\n",
    "df_topic_keywords = pd.DataFrame(lda.components_)\n",
    "# df_topic_keywords\n",
    "df_topic_keywords.columns = vec.get_feature_names()\n",
    "df_topic_keywords.index = topic_name\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['sparks', 'offers', 'card', 'email', 'get', 'order', 'unable',\n",
       "        'cant', 'tried', 'access', 'page', 'use', 'trying', 'account',\n",
       "        'offer', 'website', 'cannot', 'time', 'address', 'enter'],\n",
       "       dtype='<U167'),\n",
       " array(['order', 'delivery', 'delivered', 'easy', 'day', 'find', 'website',\n",
       "        'good', 'store', 'flowers', 'site', 'ordered', 'th', 'offer',\n",
       "        'free', 'use', 'food', 'pictures', 'collect', 'today'],\n",
       "       dtype='<U167'),\n",
       " array(['vegetarian', 'meal', 'deal', 'dine', 'main', 'options', 'option',\n",
       "        'mains', 'meat', 'fish', 'menu', 'poor', 'difficult', 'frozen',\n",
       "        'vegan', 'reach', 'eat', 'gluten', 'crashes', 'confusing'],\n",
       "       dtype='<U167'),\n",
       " array(['slow', 'friendly', 'loading', 'user', 'pages', 'rubbish', 'keeps',\n",
       "        'terrible', 'filters', 'review', 'hopeless', 'website', 'tick',\n",
       "        'virtual', 'half', 'work', 'unresponsive', 'extremely', 'jumping',\n",
       "        'cms'], dtype='<U167'),\n",
       " array(['stock', 'size', 'items', 'find', 'filters', 'available',\n",
       "        'looking', 'trousers', 'sizes', 'buy', 'dont', 'work', 'item',\n",
       "        'would', 'want', 'one', 'website', 'like', 'ms', 'sale'],\n",
       "       dtype='<U167')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Topic0</td>\n",
       "      <td>sparks</td>\n",
       "      <td>offers</td>\n",
       "      <td>card</td>\n",
       "      <td>email</td>\n",
       "      <td>get</td>\n",
       "      <td>order</td>\n",
       "      <td>unable</td>\n",
       "      <td>cant</td>\n",
       "      <td>tried</td>\n",
       "      <td>access</td>\n",
       "      <td>...</td>\n",
       "      <td>find</td>\n",
       "      <td>site</td>\n",
       "      <td>fitting</td>\n",
       "      <td>survey</td>\n",
       "      <td>bra</td>\n",
       "      <td>password</td>\n",
       "      <td>log</td>\n",
       "      <td>wont</td>\n",
       "      <td>points</td>\n",
       "      <td>see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic1</td>\n",
       "      <td>order</td>\n",
       "      <td>delivery</td>\n",
       "      <td>delivered</td>\n",
       "      <td>easy</td>\n",
       "      <td>day</td>\n",
       "      <td>find</td>\n",
       "      <td>website</td>\n",
       "      <td>good</td>\n",
       "      <td>store</td>\n",
       "      <td>flowers</td>\n",
       "      <td>...</td>\n",
       "      <td>arrived</td>\n",
       "      <td>gift</td>\n",
       "      <td>birthday</td>\n",
       "      <td>still</td>\n",
       "      <td>complicated</td>\n",
       "      <td>track</td>\n",
       "      <td>ms</td>\n",
       "      <td>days</td>\n",
       "      <td>useless</td>\n",
       "      <td>next</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic2</td>\n",
       "      <td>vegetarian</td>\n",
       "      <td>meal</td>\n",
       "      <td>deal</td>\n",
       "      <td>dine</td>\n",
       "      <td>main</td>\n",
       "      <td>options</td>\n",
       "      <td>option</td>\n",
       "      <td>mains</td>\n",
       "      <td>meat</td>\n",
       "      <td>fish</td>\n",
       "      <td>...</td>\n",
       "      <td>vegetarians</td>\n",
       "      <td>dissapointing</td>\n",
       "      <td>sports</td>\n",
       "      <td>two</td>\n",
       "      <td>mattress</td>\n",
       "      <td>navigation</td>\n",
       "      <td>course</td>\n",
       "      <td>dishes</td>\n",
       "      <td>yes</td>\n",
       "      <td>authorisation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic3</td>\n",
       "      <td>slow</td>\n",
       "      <td>friendly</td>\n",
       "      <td>loading</td>\n",
       "      <td>user</td>\n",
       "      <td>pages</td>\n",
       "      <td>rubbish</td>\n",
       "      <td>keeps</td>\n",
       "      <td>terrible</td>\n",
       "      <td>filters</td>\n",
       "      <td>review</td>\n",
       "      <td>...</td>\n",
       "      <td>responding</td>\n",
       "      <td>photographs</td>\n",
       "      <td>engine</td>\n",
       "      <td>hats</td>\n",
       "      <td>nightdress</td>\n",
       "      <td>appropriate</td>\n",
       "      <td>helpful</td>\n",
       "      <td>knew</td>\n",
       "      <td>table</td>\n",
       "      <td>bloody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Topic4</td>\n",
       "      <td>stock</td>\n",
       "      <td>size</td>\n",
       "      <td>items</td>\n",
       "      <td>find</td>\n",
       "      <td>filters</td>\n",
       "      <td>available</td>\n",
       "      <td>looking</td>\n",
       "      <td>trousers</td>\n",
       "      <td>sizes</td>\n",
       "      <td>buy</td>\n",
       "      <td>...</td>\n",
       "      <td>time</td>\n",
       "      <td>filter</td>\n",
       "      <td>back</td>\n",
       "      <td>wanted</td>\n",
       "      <td>page</td>\n",
       "      <td>cant</td>\n",
       "      <td>look</td>\n",
       "      <td>get</td>\n",
       "      <td>see</td>\n",
       "      <td>long</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1          2      3        4          5        6   \\\n",
       "Topic0      sparks    offers       card  email      get      order   unable   \n",
       "Topic1       order  delivery  delivered   easy      day       find  website   \n",
       "Topic2  vegetarian      meal       deal   dine     main    options   option   \n",
       "Topic3        slow  friendly    loading   user    pages    rubbish    keeps   \n",
       "Topic4       stock      size      items   find  filters  available  looking   \n",
       "\n",
       "              7        8        9   ...           20             21        22  \\\n",
       "Topic0      cant    tried   access  ...         find           site   fitting   \n",
       "Topic1      good    store  flowers  ...      arrived           gift  birthday   \n",
       "Topic2     mains     meat     fish  ...  vegetarians  dissapointing    sports   \n",
       "Topic3  terrible  filters   review  ...   responding    photographs    engine   \n",
       "Topic4  trousers    sizes      buy  ...         time         filter      back   \n",
       "\n",
       "            23           24           25       26      27       28  \\\n",
       "Topic0  survey          bra     password      log    wont   points   \n",
       "Topic1   still  complicated        track       ms    days  useless   \n",
       "Topic2     two     mattress   navigation   course  dishes      yes   \n",
       "Topic3    hats   nightdress  appropriate  helpful    knew    table   \n",
       "Topic4  wanted         page         cant     look     get      see   \n",
       "\n",
       "                   29  \n",
       "Topic0            see  \n",
       "Topic1           next  \n",
       "Topic2  authorisation  \n",
       "Topic3         bloody  \n",
       "Topic4           long  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show_topic(vectorize = vec, model = lda, n_words =20):\n",
    "    keywords = np.array(vec.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda.components_:\n",
    "        topic_keywords_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(topic_keywords_locs))\n",
    "    return topic_keywords\n",
    " \n",
    "show_topic(vec, lda, 20)\n",
    "\n",
    "df = pd.DataFrame(show_topic(vec, lda, 30), index = topic_name)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hope I could say that I lekid the movie {'neg': 0.0, 'neu': 0.756, 'pos': 0.244, 'compound': 0.4404}\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "def print_sentiment_scores(sentence):\n",
    "    snt = analyser.polarity_scores(sentence)\n",
    "    print(\"{:-<40} {}\".format(sentence, str(snt)))\n",
    "    \n",
    "sample_Sent = \"I just got a call from my boss - does he realise it's saturday??? :(\"\n",
    "sample_Sent1 = \"The intent behind the movie was great, but it could have been better\"\n",
    "sample_Sent2 = \"I hope I could say that I liked the movie\"\n",
    "print_sentiment_scores(sample_Sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('caesar', 'message', 'service', 'the')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spelling correction\n",
    "# !pip install autocorrect\n",
    "from autocorrect import Speller\n",
    "spell = Speller(lang='en')\n",
    "\n",
    "spell('caaaar'), spell('mussage'), spell('survice'), spell('hte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'indexer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-58467278977b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# !pip install spellchecker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mspellchecker\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSpellChecker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mspellcheck\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSpellChecker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Find those words that may be misspelled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spellchecker\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# -*- coding: utf-8 -*-\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m  \u001b[0mspellchecker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSpellchecker\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgetInstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spellchecker\\core.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mindexer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDictionaryIndex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlangdetect\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_detect_lang\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'indexer'"
     ]
    }
   ],
   "source": [
    "# !pip install spellchecker\n",
    "from spellchecker import SpellChecker\n",
    "spellcheck = SpellChecker()\n",
    "\n",
    "# Find those words that may be misspelled \n",
    "misspelled = spellcheck.unknown([\"cmputr\", \"watr\", \"study\", \"wrte\"]) \n",
    "  \n",
    "for word in misspelled: \n",
    "    # Get the one `most likely` answer \n",
    "    print(spell.correction(word)) \n",
    "  \n",
    "    # Get a list of `likely` options \n",
    "    print(spell.candidates(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text: cmputr\n",
      "corrected text: computer\n"
     ]
    }
   ],
   "source": [
    "#! pip install TextBlob\n",
    "from textblob import TextBlob \n",
    "  \n",
    "a = \"cmputr\"\n",
    "print(\"original text: \"+str(a)) \n",
    "  \n",
    "b = TextBlob(a) \n",
    "print(\"corrected text: \"+str(b.correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
